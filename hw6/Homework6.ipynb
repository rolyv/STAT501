{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "a) $SSR(X_3 | X_1) = SSE(X_1) - SSE(X_1, X_3) = 510 - 500 = 10$\n",
    "\n",
    "b) The quantity in part (a) represents the reduction in the error sum of squares that resulted when adding $X_3$ to the model when $X_1$ was already in the model.\n",
    "\n",
    "c) $SSR(X_1 | X_2, X_3) = SSE(X_2, X_3) - SSE(X_1, X_2, X_3) = 640 - 330 = 310$\n",
    "\n",
    "d) The quantity in part (c) represents the reduction in the error sum of squares that resulted when adding $X_1$ to the model when $X_2$ and $X_3$ were already in the model.\n",
    "\n",
    "e) $Y_i = \\beta_0 + \\beta_1X_{i,1} + \\epsilon_i$\n",
    "\n",
    "f) $F^* = \\frac{SSE(X_1) - SSE(X_1, X_2, X_3)}{(n - 2) - (n - 4)} \\div \\frac{SSE(X_1, X_2, X_3)}{n - 4} = \\frac{510 - 330}{(70 - 2) - (70 - 4)} \\div \\frac{330}{70 - 4} = 18$\n",
    "\n",
    "g) $R^2_{Y,2|1} = \\frac{SSE(X_1) - SSE(X_1, X_2)}{SSE(X_1)} = \\frac{510 - 450}{510} = 0.1176$\n",
    "\n",
    "h) The value in part (g) represents the marginal contribution of $X_2$ given that $X_1$ is already in the model. It shows that about 11.7% of the variation in $Y$ is reduced after adding $X_2$ to the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "a) \n",
    "\n",
    "| Source | df | Seq SS | Adj SS | F-statistic based on Adj SS | p-value based on Adj SS |\n",
    "|--------|----|--------|--------|-----------------------------|-------------------------|\n",
    "| Regression| 3 | 100.866 | 100.866 | 35.14 | 0.000 | \n",
    "| $X_1$  | 1  | 67.444 | 33.031 | 34.52 | 0.000 |\n",
    "| $X_2$  | 1  | 3.883  | 0.160  | 0.167 | 0.684 |\n",
    "| $X_3$  | 1  | 29.539  | 29.539  | 30.88 | 0.000 |\n",
    "| Error  | 93 | 88.976  | 88.976  | ----- | ----- |\n",
    "| Total  | 96 | 189.842 | 189.842 | ----- | ----- |\n",
    "\n",
    "Coefficients\n",
    "\n",
    "|Term     | Coef    | SE coef   | t-statistic | p-value |\n",
    "|---------|---------|-----------|-------------|---------|\n",
    "| Constant | 0.58    | 1.24      | 0.45        | 0.652   |\n",
    "| $X_1$   | 0.34    | 0.058     | 5.88        | 0.000   |\n",
    "| $X_2$   | -0.01   | 0.0245    | âˆ’0.408      | 0.342   |\n",
    "| $X_3$   | 0.06    | 0.0103    | 5.56        | 0.000   |\n",
    "\n",
    "b) Calculate $SSR(X_3 | X_1)$:\n",
    "\n",
    "$SSR(X_1, X_2, X_3) = SSR(X_1) + SSR(X_3 | X_1) + SSR(X_2 | X_1, X_3)$\n",
    "$SSR(X_3 | X_1) = SSR(X_1, X_2, X_3) - SSR(X_1) - SSR(X_2 | X_1, X_3) = 100.866 - 67.444 - 0.160 = 33.262$\n",
    "\n",
    "c) One way to think of the difference between the sequential sum of squares and the adjusted sum of squares is that in the sequential sum of squares, the order matters, whereas in the adjusted sum of squares, order doesn't matter. In other words, when looking at the ANOVA table, the sequential sum of squares can be thought of as a snowball that grows as you go down the list of predictors. Each line computes the sum of square of that given predictor assuming the previous predictors are already in the model, but not the ones that follow. In the case of $X_2$, the sequential sum of square only assumed the presence of $X_1$. The adjusted sum of square looks at each predictor assuming every other predictor is already in the model. In the case of $X_2$, we assumed that $X_1$ and $X_3$ were already present.\n",
    "\n",
    "d) $F^* = \\frac{SSR(X_2, X_3 | X_1)}{p - q} \\div \\frac{SSE(X_1, X_2, X_3)}{n - p} = \\frac{SSR(X_1, X_2, X_3) - SSR(X_1)}{4 - 2} \\div \\frac{SSE(X_1, X_2, X_3)}{97 - 4} = \\frac{100.866 - 67.444}{2} \\div \\frac{88.976}{93} = 17.467$\n",
    "\n",
    "e) $R^2_{Y,2|1} = \\frac{SSR(X_2 | X_1)}{SSE(X_1)} = \\frac{3.883}{122.398} = 0.0317$\n",
    "\n",
    "f) The value in part (e) represents the marginal contribution of $X_2$ given that $X_1$ is already in the model. It shows that about 3.2% of the variation in $Y$ is reduced after adding $X_2$ to the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "a) Analysis of Variance\n",
    "\n",
    "|Source     | DF  | Seq SS |\n",
    "|-----------|-----|--------|\n",
    "|Regression |  3  |2176606 |\n",
    "|  X1       |  1  | 136366 |\n",
    "|  X3       |  1  |2033565 |\n",
    "|  X2       |  1  |   6675 |\n",
    "|Error      | 48  | 985530 |\n",
    "|Total      | 51  |3162136 |\n",
    "\n",
    "b) \n",
    "\n",
    "$H_0: \\beta_2 = 0$ <br>\n",
    "$H_a: \\beta_2 \\ne 0$ <br>\n",
    "\n",
    "Full model SSE: 985530\n",
    "Reduced model SSE: 992204\n",
    "\n",
    "$F^* = \\frac{SSR(X_2 | X_1, X_3)}{1} \\div \\frac{SSE(X_1, X_2, X_2)}{52 - 4} = \\frac{6675}{1} \\div \\frac{985530}{48} = 0.3251$\n",
    "$p-value = P(X > 0.3251) = 0.571$\n",
    "\n",
    "Because the *p*-value is greater than $\\alpha = 0.05$, we conclude that we can drop $X_2$ from the model.\n",
    "\n",
    "c) This is the same hypothesis as part (b). The t-statistic is $-13.2 / 23.1 \\approx -0.57$ with a *p*-value of 0.571. The conclusion is the same as above, that we fail to reject the null hypothesis. \n",
    "\n",
    "d) $F^* = \\frac{SSR(X_2, X_3 | X_1)}{2} \\div \\frac{SSE(X_1, X_2, X_3)}{48} = \\frac{SSR(X_1, X_2, X_3) - SSR(X_1)}{2} \\div \\frac{SSE(X_1, X_2, X_3)}{48} = \\frac{2176606 - 136366}{2} \\div \\frac{985530}{48} = 49.685$\n",
    "\n",
    "*p*-value is 0.000. Therefore we can reject the null hypothesis and conclude that at least one $\\beta_i$ (for $i = 2,3$) is not equal to 0. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
